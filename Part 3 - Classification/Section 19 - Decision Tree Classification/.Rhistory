x=read.csv("Sandler - Página1.csv")
vetor=(x$NOTAS)
y=sum(vetor)
print(y)
hist(vetor)
x=read.csv("Sandler - Pagina1.csv")
vetor=(x$NOTAS)
y=sum(vetor)
print(y)
hist(vetor)
x=read.csv("Sandler - Pagina1.csv")
vetor=(x$NOTAS)
y=sum(vetor)
print(y)
hist(vetor)
x=read.csv("Sandler - Pagina1.csv")
vetor=(x$NOTAS)
y=sum(vetor)
print(y)
source('C:/Users/gabri/Desktop/projeto.R')
x=read.csv("arq.csv")
vetor=(x$NOTAS)
y=sum(vetor)
print(y)
getwd()
x=read.csv("Sandler - Pagina1.csv")
vetor=(x$NOTAS)
y=sum(vetor)
print(y)
x=read.csv("Sandler - Pagina1.csv")
vetor=(y$NOTAS)
y=sum(vetor)
print(y)
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
x=read.csv("Sandler - Pagina1.csv")
notas=(x$NOTAS)
media = 0
qtd = 0
for(e in vetor){
media = media + e
qtd = qtd + 1
}
print(media/qtd)
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
netwc()
getwc()
getwd()
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/gos2_pvsa.R')
source('C:/Users/gabri/Desktop/gos2_pvsa.R')
for (j in 1990:2020) {
anosExt = c(anosExt, j) # utilizaremos esse vetor para argumento do histograma
# nos permitira criar colunas mais precisas (breaks)
}
source('C:/Users/gabri/Desktop/gos2_pvsa.R')
source('C:/Users/gabri/Desktop/gos2_pvsa.R')
source('C:/Users/gabri/Desktop/gos2_pvsa.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
x=read.csv("Sandler - Página1.csv")
notas=(x$NOTAS)
ano=(x$ANO)
tit=(x$TITULOS)
# início da questão (2) ---------------------------------
# nessa questão, o problema da média foi resolvido da seguinte forma criando uma variável média a ser incrementada com cada
# valor do vetor de notas dividida pela váriável qtd que corresponde ao total de elementos do vetor
media = 0
qtd = 0
for(e in notas){
media = media + e
qtd = qtd + 1
}
print(media/qtd)
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
source('C:/Users/gabri/Desktop/projeto.R')
clear
clear()
source('C:/Users/gabri/Desktop/gos2_pvsa.R')
source('C:/Users/gabri/Desktop/gos2_pvsa.R')
source('C:/Users/gabri/Desktop/gos2_pvsa.R')
source('C:/Users/gabri/Desktop/gos2_pvsa.R')
source('C:/Users/gabri/Desktop/gos2_pvsa.R')
source('C:/Users/gabri/Desktop/gos2_pvsa.R')
setwd("~/mlaz/Part 3 - Classification/Section 18 - Naive Bayes")
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
View(dataset)
View(dataset)
setwd("~/mlaz/Part 3 - Classification/Section 19 - Decision Tree Classification")
# Decision Tree Classification
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting Decision Tree Classification to the Training set
install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
